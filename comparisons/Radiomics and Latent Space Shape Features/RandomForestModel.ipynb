{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 13:23:37.884900: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-10 13:23:37.884941: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/rfyahya/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner -U -qq\n",
    "import keras_tuner as kt\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "try:\n",
    "  from wurlitzer import sys_pipes\n",
    "except:\n",
    "  from colabtools.googlelog import CaptureLog as sys_pipes\n",
    "\n",
    "from IPython.core.magic import register_line_magic\n",
    "from IPython.display import Javascript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found TensorFlow Decision Forests v0.2.7\n"
     ]
    }
   ],
   "source": [
    "# Check the version of TensorFlow Decision Forests\n",
    "print(\"Found TensorFlow Decision Forests v\" + tfdf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "def delong_auc(truth, y_pred):\n",
    "    auc_delong, auc_cov = delong_roc_variance(\n",
    "    truth,\n",
    "    y_pred)\n",
    "    return auc_delong\n",
    "\n",
    "def delong_auc_cov(truth, y_pred):\n",
    "    auc_delong, auc_cov = delong_roc_variance(\n",
    "    truth,\n",
    "    y_pred)\n",
    "    return auc_cov\n",
    "\n",
    "def delong_ci(truth, y_pred):\n",
    "    auc_delong, auc_cov = delong_roc_variance(\n",
    "    truth,\n",
    "    y_pred)\n",
    "\n",
    "    alpha = .95\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(\n",
    "        lower_upper_q,\n",
    "        loc=auc_delong,\n",
    "        scale=auc_std)\n",
    "\n",
    "    ci[ci > 1] = 1\n",
    "\n",
    "    return ci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Specificity(tn, fp):\n",
    "    return (tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 13:24:29.902451: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-10 13:24:29.902619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-10 13:24:29.902726: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-10 13:24:29.951869: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-10 13:24:29.952114: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-10 13:24:29.952225: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-08-10 13:24:29.952239: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-08-10 13:24:29.953015: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/home/rfyahya/Downloads/test_RAD.csv\")\n",
    "test = test[test.B]\n",
    "train = pd.read_csv(\"/home/rfyahya/Downloads/Train_RAD.csv\")\n",
    "val = pd.read_csv(\"/home/rfyahya/Downloads/Val_RAD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = final_test[final_test.BraTS21ID != 123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1000 Complete [00h 00m 03s]\n",
      "val_accuracy: 0.3617021143436432\n",
      "\n",
      "Best val_accuracy So Far: 0.7021276354789734\n",
      "Total elapsed time: 00h 36m 26s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'min_examples': 5, 'categorical_algorithm': 'CART', 'max_depth': 35, 'use_hessian_gain': 1, 'shrinkage': 0.27, 'num_candidate_attributes_ratio': 0.7}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters tuning: Using grid search, we tune two hyperparameters of the model; the number of samples to split a node and maximum depth of trees.\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load your data here\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a random forest classifier model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Train the model on the grid search object\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and model score\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The `num_threads` constructor argument is not set and the number of CPU is os.cpu_count()=32 > 32. Setting num_threads to 32. Set num_threads manually to use more than 32 cpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `num_threads` constructor argument is not set and the number of CPU is os.cpu_count()=32 > 32. Setting num_threads to 32. Set num_threads manually to use more than 32 cpus.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmphardcgdb as temporary training directory\n",
      "Reading training dataset...\n",
      "Training tensor examples:\n",
      "Features: {'BraTS21ID': <tf.Tensor 'data:0' shape=(None,) dtype=int64>, 'Skewness_Core_T1w': <tf.Tensor 'data_5:0' shape=(None,) dtype=float64>, 'Energy_Oedema_T1W': <tf.Tensor 'data_1:0' shape=(None,) dtype=float64>, 'GLCM_Contrast_Necrosis_FLAIR': <tf.Tensor 'data_2:0' shape=(None,) dtype=float64>, 'GLSZM_Grey_Level_Variance_Enhanced_T1c': <tf.Tensor 'data_3:0' shape=(None,) dtype=float64>, 'GLSZM_Low_Grey_Level_Zone_Emphasis_Oedema_T2w': <tf.Tensor 'data_4:0' shape=(None,) dtype=float64>, 'original_ngtdm_Busyness_Core_T2w': <tf.Tensor 'data_6:0' shape=(None,) dtype=float64>}\n",
      "Label: Tensor(\"data_7:0\", shape=(None,), dtype=int64)\n",
      "Weights: None\n",
      "Normalized tensor features:\n",
      " {'BraTS21ID': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast:0' shape=(None,) dtype=float32>), 'Skewness_Core_T1w': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_1:0' shape=(None,) dtype=float32>), 'Energy_Oedema_T1W': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_2:0' shape=(None,) dtype=float32>), 'GLCM_Contrast_Necrosis_FLAIR': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_3:0' shape=(None,) dtype=float32>), 'GLSZM_Grey_Level_Variance_Enhanced_T1c': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_4:0' shape=(None,) dtype=float32>), 'GLSZM_Low_Grey_Level_Zone_Emphasis_Oedema_T2w': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_5:0' shape=(None,) dtype=float32>), 'original_ngtdm_Busyness_Core_T2w': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_6:0' shape=(None,) dtype=float32>)}\n",
      "Training dataset read in 0:00:00.458133. Found 420 examples.\n",
      "Training model...\n",
      "Standard output detected as not visible to the user e.g. running in a notebook. Creating a training log redirection. If training get stuck, try calling tfdf.keras.set_training_logs_redirection(False).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO kernel.cc:813] Start Yggdrasil model training\n",
      "[INFO kernel.cc:814] Collect training examples\n",
      "[INFO kernel.cc:422] Number of batches: 1\n",
      "[INFO kernel.cc:423] Number of examples: 420\n",
      "[INFO kernel.cc:836] Training dataset:\n",
      "Number of records: 420\n",
      "Number of columns: 8\n",
      "\n",
      "Number of columns by type:\n",
      "\tNUMERICAL: 7 (87.5%)\n",
      "\tCATEGORICAL: 1 (12.5%)\n",
      "\n",
      "Columns:\n",
      "\n",
      "NUMERICAL: 7 (87.5%)\n",
      "\t0: \"BraTS21ID\" NUMERICAL mean:435.324 min:0 max:1010 sd:248.179\n",
      "\t1: \"Energy_Oedema_T1W\" NUMERICAL mean:9.70047e+10 min:7.11313e+07 max:1.27453e+12 sd:1.76999e+11\n",
      "\t2: \"GLCM_Contrast_Necrosis_FLAIR\" NUMERICAL mean:589.752 min:0 max:7487.18 sd:920.433\n",
      "\t3: \"GLSZM_Grey_Level_Variance_Enhanced_T1c\" NUMERICAL mean:16787.3 min:12.7231 max:136993 sd:24510.3\n",
      "\t4: \"GLSZM_Low_Grey_Level_Zone_Emphasis_Oedema_T2w\" NUMERICAL mean:0.000715942 min:2.39691e-05 max:0.0185257 sd:0.00131884\n",
      "\t5: \"Skewness_Core_T1w\" NUMERICAL mean:0.127504 min:-4.53399 max:2.02957 sd:0.800462\n",
      "\t6: \"original_ngtdm_Busyness_Core_T2w\" NUMERICAL mean:0.0295133 min:0.00046436 max:0.370357 sd:0.0385854\n",
      "\n",
      "CATEGORICAL: 1 (12.5%)\n",
      "\t7: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n",
      "\n",
      "Terminology:\n",
      "\tnas: Number of non-available (i.e. missing) values.\n",
      "\tood: Out of dictionary.\n",
      "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
      "\ttokenized: The attribute value is obtained through tokenization.\n",
      "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
      "\tvocab-size: Number of unique values.\n",
      "\n",
      "[INFO kernel.cc:882] Configure learner\n",
      "[WARNING gradient_boosted_trees.cc:1671] Subsample hyperparameter given but sampling method does not match.\n",
      "[WARNING gradient_boosted_trees.cc:1684] GOSS alpha hyperparameter given but GOSS is disabled.\n",
      "[WARNING gradient_boosted_trees.cc:1693] GOSS beta hyperparameter given but GOSS is disabled.\n",
      "[WARNING gradient_boosted_trees.cc:1705] SelGB ratio hyperparameter given but SelGB is disabled.\n",
      "[INFO kernel.cc:912] Training config:\n",
      "learner: \"GRADIENT_BOOSTED_TREES\"\n",
      "features: \"BraTS21ID\"\n",
      "features: \"Energy_Oedema_T1W\"\n",
      "features: \"GLCM_Contrast_Necrosis_FLAIR\"\n",
      "features: \"GLSZM_Grey_Level_Variance_Enhanced_T1c\"\n",
      "features: \"GLSZM_Low_Grey_Level_Zone_Emphasis_Oedema_T2w\"\n",
      "features: \"Skewness_Core_T1w\"\n",
      "features: \"original_ngtdm_Busyness_Core_T2w\"\n",
      "label: \"__LABEL\"\n",
      "task: CLASSIFICATION\n",
      "random_seed: 123456\n",
      "metadata {\n",
      "  framework: \"TF Keras\"\n",
      "}\n",
      "pure_serving_model: false\n",
      "[yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n",
      "  num_trees: 300\n",
      "  decision_tree {\n",
      "    max_depth: 35\n",
      "    min_examples: 5\n",
      "    in_split_min_examples_check: true\n",
      "    keep_non_leaf_label_distribution: true\n",
      "    missing_value_policy: GLOBAL_IMPUTATION\n",
      "    allow_na_conditions: false\n",
      "    categorical_set_greedy_forward {\n",
      "      sampling: 0.1\n",
      "      max_num_items: -1\n",
      "      min_item_frequency: 1\n",
      "    }\n",
      "    growing_strategy_local {\n",
      "    }\n",
      "    categorical {\n",
      "      cart {\n",
      "      }\n",
      "    }\n",
      "    num_candidate_attributes_ratio: 0.7\n",
      "    axis_aligned_split {\n",
      "    }\n",
      "    internal {\n",
      "      sorting_strategy: PRESORTED\n",
      "    }\n",
      "    uplift {\n",
      "      min_examples_in_treatment: 5\n",
      "      split_score: KULLBACK_LEIBLER\n",
      "    }\n",
      "  }\n",
      "  shrinkage: 0.27\n",
      "  loss: DEFAULT\n",
      "  validation_set_ratio: 0.1\n",
      "  validation_interval_in_trees: 1\n",
      "  early_stopping: VALIDATION_LOSS_INCREASE\n",
      "  early_stopping_num_trees_look_ahead: 30\n",
      "  l2_regularization: 0\n",
      "  lambda_loss: 1\n",
      "  mart {\n",
      "  }\n",
      "  adapt_subsample_for_maximum_training_duration: false\n",
      "  l1_regularization: 0\n",
      "  use_hessian_gain: true\n",
      "  l2_regularization_categorical: 1\n",
      "  apply_link_function: true\n",
      "  compute_permutation_variable_importance: false\n",
      "  binary_focal_loss_options {\n",
      "    misprediction_exponent: 2\n",
      "    positive_sample_coefficient: 0.5\n",
      "  }\n",
      "}\n",
      "\n",
      "[INFO kernel.cc:915] Deployment config:\n",
      "cache_path: \"/tmp/tmphardcgdb/working_cache\"\n",
      "num_threads: 32\n",
      "try_resume_training: true\n",
      "\n",
      "[INFO kernel.cc:944] Train model\n",
      "[INFO gradient_boosted_trees.cc:405] Default loss set to BINOMIAL_LOG_LIKELIHOOD\n",
      "[INFO gradient_boosted_trees.cc:1008] Training gradient boosted tree on 420 example(s) and 7 feature(s).\n",
      "[INFO gradient_boosted_trees.cc:1051] 378 examples used for training and 42 examples used for validation\n",
      "[INFO gradient_boosted_trees.cc:1434] \tnum-trees:1 train-loss:1.126853 train-accuracy:0.828042 valid-loss:1.343144 valid-accuracy:0.547619\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:2 train-loss:0.927827 train-accuracy:0.925926 valid-loss:1.353525 valid-accuracy:0.642857\n",
      "[INFO gradient_boosted_trees.cc:2871] Early stop of the training because the validation loss does not decrease anymore. Best valid-loss: 1.34314\n",
      "[INFO gradient_boosted_trees.cc:1479] Create final snapshot of the model at iteration 30\n",
      "[INFO gradient_boosted_trees.cc:230] Truncates the model to 1 tree(s) i.e. 1  iteration(s).\n",
      "[INFO gradient_boosted_trees.cc:264] Final model num-trees:1 valid-loss:1.343144 valid-accuracy:0.547619\n",
      "[INFO kernel.cc:961] Export model in log directory: /tmp/tmphardcgdb with prefix 9233c7a6fe7b49f6\n",
      "[INFO kernel.cc:978] Save model in resources\n",
      "[INFO kernel.cc:1176] Loading model from path /tmp/tmphardcgdb/model/ with prefix 9233c7a6fe7b49f6\n",
      "[INFO abstract_model.cc:1248] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n",
      "[INFO kernel.cc:1022] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 0:00:00.267066\n",
      "Compiling model...\n",
      "Model compiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad10d46940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train best model\n",
    "\n",
    "best_hyper_parameters[\"use_hessian_gain\"] = bool(best_hyper_parameters[\"use_hessian_gain\"])\n",
    "best_model = tfdf.keras.GradientBoostedTreesModel(**best_hyper_parameters)\n",
    "best_model.compile(metrics=[METRICS])\n",
    "best_model.fit(train_ds, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At test time, given an unseen subject, the 117 models are all used to predict the\n",
    "#MGMT methylation status, each predicting either 0 or 1 for the unmethylated\n",
    "#or methylated group, respectively. The average of the predictions is interpreted\n",
    "#as the probability of belonging to the methylated group.\n",
    "\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(final_test, label=\"MGMT_value\") \n",
    "\n",
    "evaluation = best_model.evaluate(test_ds, return_dict=True)\n",
    "print()\n",
    "\n",
    "for name, value in evaluation.items():\n",
    "  print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "preds = best_model.predict(test_ds, verbose=1)\n",
    "truth = np.array(list(final_test.MGMT_value))\n",
    "print(truth.shape)\n",
    "print(preds.shape)\n",
    "print(\"AUC = \", delong_auc(truth, preds.squeeze()))\n",
    "print(\"conv = \", delong_auc_cov(truth, preds.squeeze()))\n",
    "print(\"ci = \", delong_ci(truth, preds.squeeze()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6181818181818182"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Specificity(34, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5812118635350592"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2*0.6182* 0.5484)/(0.6182+ 0.5484)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
